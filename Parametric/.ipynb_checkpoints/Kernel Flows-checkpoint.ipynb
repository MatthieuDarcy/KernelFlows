{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from KF_parametric_autograd import GPRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd648e2",
   "metadata": {},
   "source": [
    "# The regression problem\n",
    "\n",
    "\n",
    "Suppose we have noisy observations of an unknwon function\n",
    "\n",
    "$$ y_i = f(x_i) + \\varepsilon_i$$\n",
    "wehere $\\varepsilon_i$ is i.i.d noise. We wish to recover the underlying function $f$. \n",
    "\n",
    "In Kernel Methods and Gaussian processes regression, the solution to the above problem has the representer representation\n",
    "\n",
    "$$ f^* = k(\\cdot, X)(K(X,X) + \\lambda I)^{-1}Y$$\n",
    "\n",
    "where $X = (x_i)_{i=1}, Y = (y_i)_{i=1}$ are the vectors of observations and $k$ is a kernel function parametrized by some parameters $\\theta$.\n",
    "\n",
    "# Choosing the best kernel\n",
    "\n",
    "The above solution relies on a choice of kernel family $k$ and a choice of parameters $\\theta$. Here we focus on the problem of given a kernel family, choose the optimal parameter $\\theta$.\n",
    "\n",
    "The paper \"Kernel Flows: from learning kernels from data into the abyss\" by Owhadi and Yoo (https://arxiv.org/abs/1808.04475) proposes to minimize a cross validation loss by sampling a subset $(X_s, Y_s)$ from the full data set $(X, Y)$ (of roughly half the size). We then minimize the following (random) loss\n",
    "\n",
    "$$ \\rho(\\theta) = \\frac{|| u - u_s||^2}{|| u||^2 } = 1 - \\frac{Y_s^T K(X_s,X_s)^{-1}Y^s}{Y^T K(X,X)^{-1}Y}.$$\n",
    "\n",
    "The function $ u$ is the optimal function which sees the full data set $(X,Y)$ and $u_s$ is the optimal function which sees the sample $(X_s, Y_s)$. The norm is the RKHS induced by the kernel. For more details, see the following talk https://www.youtube.com/watch?v=ZndevdR4omw.\n",
    "\n",
    "# Implementation\n",
    "\n",
    "## Data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ea647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A relatively complicated 2d function\n",
    "noise_level = 0.2\n",
    "def f(x, noise_level=noise_level):\n",
    "    return np.sin(5 * x[1]) * (1 - np.tanh(x[0] ** 2)) + x[0]**3*np.exp(-x[1])\\\n",
    "    + np.random.randn() * noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2, 2, size = (100,2))\n",
    "\n",
    "fx = np.array([f(x_i, noise_level=0.0) for x_i in x])\n",
    "y = np.array([f(x_i, noise_level=noise_level) for x_i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b70703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_test, Y, Y_test = train_test_split(x,y, test_size = 0.2, random_state=2022, shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb83403",
   "metadata": {},
   "source": [
    "## Default GP regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RBF kernel\n",
    "from kernel_functions_autograd import kernel_RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1da377",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.array([1.0])\n",
    "GP =  GPRegressor(kernel_RBF, sigma)\n",
    "\n",
    "GP.fit(X,Y)\n",
    "\n",
    "pred = GP.predict(X_test)\n",
    "\n",
    "print(\"The mean squared error on the test set is\", np.round(mse(pred, Y_test), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cceca",
   "metadata": {},
   "source": [
    "## GP optimized via gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f85a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP =  GPRegressor(kernel_RBF, sigma)\n",
    "opt_param = GP.optimize_parameters(X, Y, sigma, 10000, learning_rate = 0.5, optimizer = \"Nesterov\")\n",
    "GP.fit(X, Y, parameters=opt_param)\n",
    "\n",
    "# Alternatively call (but is less flexible)\n",
    "# GP.fit(X, Y, optimize = True)\n",
    "\n",
    "pred = GP.predict(X_test)\n",
    "print(\"The optimized paramters are \", GP.parameters)\n",
    "print(\"The mean squared error on the test set is\", np.round(mse(pred, Y_test), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597797ab",
   "metadata": {},
   "source": [
    "Next we plot the rho loss function. Because rho is very noisy, it is also beneficial to plot a running average. A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e6aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(GP.rho_running_mean, label = \"Running mean\")\n",
    "plt.legend()\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(GP.rho_hist, label = \"Rho\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7428fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP.para_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47132587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
